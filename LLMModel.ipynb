{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Load Model & Tokenizer ----Loads the pre-trained \"Qwen/QwQ-32B\" model and its tokenizer.\n",
        "# Load Dataset ---Fetches the dataset and tokenizes it (truncates and pads text).\n",
        "# Set Training Parameters --- Defines training settings like batch size, epochs, and learning rate.\n",
        "# Initialize Trainer ---Creates a Trainer object to handle model training and evaluation.\n",
        "# Train Model --- Starts the training process.\n",
        "# Save Model --- Saves the fine-tuned model for later use."
      ],
      "metadata": {
        "id": "fK8hnWEN7iII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/QwQ-32B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/QwQ-32B\")"
      ],
      "metadata": {
        "id": "pqQl7Keo8vHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the dataset loading function from the datasets library\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load your dataset (replace \"your_dataset\" with the actual dataset name)\n",
        "dataset = load_dataset(\"your_dataset\")\n",
        "\n",
        "# Tokenize the dataset (apply padding and truncation to ensure uniform input size)\n",
        "tokenized_data = tokenizer(dataset[\"text\"], truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "73RVme6H9GuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import training arguments class from the transformers library\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Define training parameters such as batch size, learning rate, and output directory\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",  # Directory where model checkpoints will be saved\n",
        "    per_device_train_batch_size=4,  # Number of samples per batch for training\n",
        "    num_train_epochs=3,  # Number of times the model will see the entire dataset\n",
        "    learning_rate=5e-5,  # Learning rate for optimizing model weights\n",
        ")"
      ],
      "metadata": {
        "id": "5w_JOToR7vXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Trainer class from the transformers library\n",
        "from transformers import Trainer\n",
        "\n",
        "# Create an instance of Trainer to handle training and evaluation\n",
        "trainer = Trainer(\n",
        "    model=model,  # The model to be trained\n",
        "    args=training_args,  # Training parameters\n",
        "    train_dataset=tokenized_data,  # The tokenized training dataset\n",
        "    eval_dataset=tokenized_data_val,  # The tokenized validation dataset (ensure this is defined)\n",
        ")\n",
        "\n",
        "# Start training the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "0ob76crJ7_BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model to the specified directory\n",
        "trainer.save_model(\"fine-tuned-llm\")"
      ],
      "metadata": {
        "id": "9Xo8ss_78AO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/QwQ-32B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/QwQ-32B\")\n",
        "\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"your_dataset\")\n",
        "tokenized_data = tokenizer(dataset[\"text\"], truncation=True, padding=True)\n",
        "\n",
        "\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=5e-5,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "from transformers import Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data,  # Training dataset\n",
        "    eval_dataset=tokenized_data_val,  # Validation dataset\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "trainer.save_model(\"fine-tuned-llm\")"
      ],
      "metadata": {
        "id": "aPMyQ7488MEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import Required Libraries\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# 2. Load Pre-trained Model and Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/QwQ-32B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/QwQ-32B\")\n",
        "\n",
        "# 3. Load and Tokenize Dataset\n",
        "dataset = load_dataset(\"your_dataset\")\n",
        "tokenized_data = tokenizer(dataset[\"text\"], truncation=True, padding=True)\n",
        "\n",
        "# 4. Define Training Parameters\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",  # Save checkpoints here\n",
        "    per_device_train_batch_size=4,  # Training batch size\n",
        "    num_train_epochs=3,  # Number of epochs\n",
        "    learning_rate=5e-5,  # Learning rate\n",
        ")\n",
        "\n",
        "# 5. Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data,\n",
        "    eval_dataset=tokenized_data_val,  # Ensure this is defined\n",
        ")\n",
        "\n",
        "# 6. Train the Model\n",
        "trainer.train()\n",
        "\n",
        "# 7. Save the Fine-Tuned Model\n",
        "trainer.save_model(\"fine-tuned-llm\")\n"
      ],
      "metadata": {
        "id": "hA-sbO8h_LEA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}